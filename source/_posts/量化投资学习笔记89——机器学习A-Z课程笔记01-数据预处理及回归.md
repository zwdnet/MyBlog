---
title: '量化投资学习笔记89——机器学习A-Z课程笔记01:数据预处理及回归'
date: 2020-09-18 09:21:34
tags: [量化投资,线性回归,网络课程,机器学习,学习笔记]
categories: 量化投资
---
网络课程，B站上有视频。我是先看的吴恩达的视频，有好多个版本，尤其是早期还用matlab写代码，看不下去来看这个。这个蛮好，讲得很细，代码都用python和R实操。打算先把这个学完。
第0部分
1.机器学习应用
略
2.机器学习是未来
数据很丰富。数据量暴增，只有用机器学习来处理。
3、4.安装开发环境
略。
5.下载数据集
https://www.superdatascience.com/pages/下载数据集
第1部分 数据预处理
6.数据预处理
导入数据
```python
pd.read_csv()
```
7.处理缺失数据
```python
from sklearn.preprocessing import Imputer
    x = dataset.iloc[:, :-1].values
    y = dataset.iloc[:, 3].values
    print(x, y)
    imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
    imputer = imputer.fit(x[:, 1:3])
    x[:, 1:3] = imputer.transform(x[:, 1:3])
    print(x)
```
8.分类数据的处理
```python
    # 将分类数据编码
    labelencoder_x = LabelEncoder()
    x[:, 0] = labelencoder_x.fit_transform(x[:, 0])
    print(x)
```
如此处理会使分类数据有大小关系，用虚拟编码解决。
将所有可能分类都作为一列，然后用0/1表示是否属于该类。
```python
    # 虚拟编码
    onehotEncoder = OneHotEncoder(categorical_features = [0])
    x = onehotEncoder.fit_transform(x).toarray()
    # 处理因变量，不是必要的
    labelencoder_y = LabelEncoder()
    y = labelencoder_y.fit_transform(y)
    print(y)
```
9.划分训练集和测试集
算法从训练集学习，用测试集来测试。
划分的原因:学习的是模型不是数据本身。
```python
    # 划分训练集和测试集
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)
    print(x_train, y_train, x_test, y_test)
```
10.特征缩放
处理不同的变量不在同一数量级上的情况，如年龄和收入。
方法有标准化(standardisation)和正态化(Normalisation)
标准化: x = [x-mean(x)]/std(x)
正态化: x = [x-min(x)]/[max(x)-min(x)]
```python
    # 特征缩放
    sc_x = StandardScaler()
    x_train = sc_x.fit_transform(x_train)
    x_test = sc_x.transform(x_test)
    print(x_train, x_test)
```
因变量是分类变量，就不必了。如果是定量变量，是可能需要的。
11.数据预处理模板
特征工程还是很重要的，也许不能靠模板，pass吧。
总结一下数据处理数据的过程:读取数据(pd.read_csv)，处理缺失数据(sklearn.preprocessing.Imputer)，对分类数据进行编码(sklearn.preprocessing.LabelEncoder, OneHotEncoder)，特征缩放(正则化，sklearn.preprocessing.StandardScaler)，划分训练集和测试集(sklearn.model_selection.train_test_split)。
第2部分 简单线性回归
一元线性回归。
1.第一步
模型:线性方程
y = b0 + b1x1
y是因变量，x1是自变量。
2.第二步
拟合线性模型
对数据集中每个点，做垂直于x轴的直线与预测直线相交，交点横坐标xi，纵坐标为预测值yi'，找到一个参数组合b0，b1，使得所有点的(yi - yi')²之和最小。
3.实操
进行数据预处理
没有缺失数据，也没有分类变量，使用sklearn线性回归也自带了特征缩放，所以直接划分训练集和测试集。
4.实操，进行线性回归
```python
    # 进行线性回归
    regress = LinearRegression()
    regress.fit(x_train, y_train)
```
5.实操，用模型进行预测
```python
    # 用模型进行预测，用测试集来预测
    y_pred = regress.predict(x_test)
    print(y_pred, y_test)
```
6.实操，对结果画图比较
先画训练集
```python
    # 结果绘图
    plt.figure()
    plt.scatter(x_train, y_train, color = "red")
    plt.plot(x_train, regress.predict(x_train), color = "blue")
    plt.title("Salary vs Expernce (trainning set)")
    plt.xlabel("Year of Expernce")
    plt.ylabel("Salary")
    plt.savefig("P2_train.png")
```
![](https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0178-QTLearn/62/01.png)
再画测试集
```python
    # 测试集结果
    plt.figure()
    plt.scatter(x_test, y_test, color = "red")
    plt.plot(x_train, regress.predict(x_train), color = "blue")
    plt.title("Salary vs Expernce (trainning set)")
    plt.xlabel("Year of Expernce")
    plt.ylabel("Salary")
    plt.savefig("P2_test.png")
```
![](https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0178-QTLearn/62/02.png)
结果不错。
多元线性回归。
1.模型
y = b0+b1x1+b2x2+……+bnxn
有多个自变量。
几个假设:
①线性
②同方差性
③多元正态分布
④误差独立
⑤无多重共线性
2.虚拟变量(dummy variables)
分类数据无法排序。如果是二分类变量，用其中一个状态的虚拟变量就可表示(非此即彼)。
虚拟变量陷阱:如果把二分类变量的两个变量都纳入模型，二者有确定性的关系，会产生多重共线性，不满足多元线性回归的假设。参数过多会有过拟合的问题。多分类变量也是如此，永远要省略掉一个虚拟变量。
3.建模步骤
要舍弃一些自变量。原因:垃圾进，垃圾出。
五种方法:
①全部变量纳入
②反向淘汰
③顺向选择
④双向淘汰
⑤信息量比较
②③④称为逐步回归。
①全部纳入
a.基于先验知识
b.必须为之
c.为反向淘汰做准备
②反向淘汰
a.选择一个显著性水平(SL)
b.用所有变量建模
c.计算各变量的P值，若最大的p值大于SL，进入第四步。否则(所有变量P值小于等于SL，结束)。
d.从模型中去除P值最高的变量。
e.用剩下的自变量重复c,d步。
③顺向淘汰
a.选择一个显著性水平(SL)
b.对所有自变量分别进行简单一元线性回归，选择P值最低的变量。
c.保留该变量，依次添加额外的一个变量进行建模。
d.如果有P值小于SL，重复第三步，否则结束。
④双向淘汰
a.选择一个进入模型和存留在模型中的显著性水平(SLENTER和SLSTAY)
b.按照顺向淘汰的方法选择一个变量进入模型(P<SLENTER)
c.按照反向淘汰的方法剔除变量(P>SLSTATY)
d.当没有变量能进入和退出时结束。
⑤信息量比较
a.选择一个方法对模型打分。(例如Akaike criterion)
b.建立所有的可能的模型(对n个自变量有2^n-1个模型)
c.选择评分最高的模型。
4.实操
读取数据，有一列分类数据。用OneHot编码转换。
然后进行建模和预测，跟一元线性回归一样的。
上面是采用全部纳入的策略，下面再用其它策略试试。
用statsmodels库来输出回归的具体结果。
多元线性回归，要加上常数项b0x0，其中x0=1。
我在手机上没法装statsmodels库，只有到云服务器里弄了。
程序运行老有问题，先跳过吧。
多项式线性回归
1.原理
y = b0+b1x1+b2x1^2+…+bnx1^n
问题，为什么称为"线性"?
指的是方程参数是不是线性组合。
2.实操
读取数据，画散点图看看。
![](https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0178-QTLearn/62/03.png)
明显是非线性的，用多项式回归。
因为数据量小，就不划分训练集和测试集。
![](https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0178-QTLearn/62/04.png)
蓝线是线性回归，绿线是二次多项式回归。可以看到好了很多，但还可以改进。升高多项式次数。
![](https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0178-QTLearn/62/05.png)
升高到三次多项式，黑色的，拟合的更好了。
再升高到四次看看。
![](https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0178-QTLearn/62/06.png)
更准了，但是也过拟合了。
平滑曲线
![](https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/blog0178-QTLearn/62/07.png)
3.评价模型
R平方值
预测值和真实值的差的平方之和称为剩余平方和(SSres)，回归即使该值最小。
预测值与真实值平均值之差的平方之和为共平方和(SStot)。
R² = 1 - SSres/SStot
范围0-1，值越大，拟合越精准。
调整R平方值(Adjusted R²)
新增加一个自变量后，R平方值不会减小。即增加自变量几乎一定会改善拟合度，但模型未必会更好。因此定义调整R平方值。
AR² = 1-(1-R²)(n-1)/(n-p-1)
p:自变量个数
n:数据个数
代码：https://github.com/zwdnet/MyQuant/tree/master/49





我发文章的三个地方，欢迎大家在朋友圈等地方分享，欢迎点“在看”。
我的个人博客地址：https://zwdnet.github.io
我的知乎文章地址： https://www.zhihu.com/people/zhao-you-min/posts
我的微信个人订阅号：赵瑜敏的口腔医学学习园地


![](https://zymblog-1258069789.cos.ap-chengdu.myqcloud.com/other/wx.jpg)